{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014874,
     "end_time": "2021-02-24T08:22:13.283150",
     "exception": false,
     "start_time": "2021-02-24T08:22:13.268276",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "dataset : https://github.com/multi30k/dataset\n",
    "<br>\n",
    "https://github.com/kh-kim/simple-nmt/blob/master/simple_nmt/models/seq2seq.py<br><br>\n",
    "https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-02-24T08:22:13.317541Z",
     "iopub.status.busy": "2021-02-24T08:22:13.316848Z",
     "iopub.status.idle": "2021-02-24T08:22:14.424517Z",
     "shell.execute_reply": "2021-02-24T08:22:14.423724Z"
    },
    "papermill": {
     "duration": 1.128089,
     "end_time": "2021-02-24T08:22:14.424728",
     "exception": false,
     "start_time": "2021-02-24T08:22:13.296639",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/multi30k-en-fr/val.en\n",
      "/kaggle/input/multi30k-en-fr/train.fr\n",
      "/kaggle/input/multi30k-en-fr/val.fr\n",
      "/kaggle/input/multi30k-en-fr/train.en\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext import data, datasets\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "\n",
    "# import simple_nmt.data_loader as data_loader\n",
    "# from simple_nmt.search import SigleBeamSearchBoard\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-24T08:22:14.456722Z",
     "iopub.status.busy": "2021-02-24T08:22:14.455716Z",
     "iopub.status.idle": "2021-02-24T08:22:14.461011Z",
     "shell.execute_reply": "2021-02-24T08:22:14.460461Z"
    },
    "papermill": {
     "duration": 0.022265,
     "end_time": "2021-02-24T08:22:14.461149",
     "exception": false,
     "start_time": "2021-02-24T08:22:14.438884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# link에서 다운 받아서 하는 방법을 모르겠다\n",
    "multi30k_path = \"/kaggle/input/multi30k-en-fr/\"\n",
    "\n",
    "train_list = [\"train.fr\",\"train.en\"]\n",
    "valid_list = [\"val.fr\",\"val.en\"]\n",
    "# test_list = [\"test_2016_flickr.fr.gz\",\"test_2016_flickr.en.gz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-24T08:22:14.496265Z",
     "iopub.status.busy": "2021-02-24T08:22:14.495610Z",
     "iopub.status.idle": "2021-02-24T08:22:14.577819Z",
     "shell.execute_reply": "2021-02-24T08:22:14.577189Z"
    },
    "papermill": {
     "duration": 0.102992,
     "end_time": "2021-02-24T08:22:14.577968",
     "exception": false,
     "start_time": "2021-02-24T08:22:14.474976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_french len is: 29000\n",
      "training_english len is: 29000\n",
      "\n",
      "validate_french len is: 1014\n",
      "validate_english len is: 1014\n"
     ]
    }
   ],
   "source": [
    "# load train data\n",
    "# 그냥 txt 파일처럼 읽어버리면 되는 거였다.\n",
    "with open(multi30k_path+train_list[0], 'rb') as fr_path:\n",
    "    fr_train = fr_path.readlines()\n",
    "with open(multi30k_path+train_list[1], 'rb') as en_path:\n",
    "    en_train = en_path.readlines()\n",
    "    \n",
    "# load validation data\n",
    "with open(multi30k_path+valid_list[0], 'rb') as fr_path:\n",
    "    fr_val = fr_path.readlines()\n",
    "with open(multi30k_path+valid_list[1], 'rb') as en_path:\n",
    "    en_val = en_path.readlines()\n",
    "    \n",
    "# test data는 트레이닝 끝난 다음에 하기\n",
    "\n",
    "print(f\"training_french len is: {len(fr_train)}\")\n",
    "print(f\"training_english len is: {len(en_train)}\")\n",
    "print()\n",
    "print(f\"validate_french len is: {len(fr_val)}\")\n",
    "print(f\"validate_english len is: {len(en_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013988,
     "end_time": "2021-02-24T08:22:14.606929",
     "exception": false,
     "start_time": "2021-02-24T08:22:14.592941",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-24T08:22:14.665355Z",
     "iopub.status.busy": "2021-02-24T08:22:14.664393Z",
     "iopub.status.idle": "2021-02-24T08:22:14.667375Z",
     "shell.execute_reply": "2021-02-24T08:22:14.666893Z"
    },
    "papermill": {
     "duration": 0.046331,
     "end_time": "2021-02-24T08:22:14.667536",
     "exception": false,
     "start_time": "2021-02-24T08:22:14.621205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "LENGTH_PENALTY = .2\n",
    "MIN_LENGTH = 5\n",
    "\n",
    "class SingleBeamSearchBoard():\n",
    "    def __init__(self, \n",
    "                 device, \n",
    "                 prev_status_config,\n",
    "                 beam_size=5,\n",
    "                max_length=255):\n",
    "        \n",
    "        self.beam_size = beam_size\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # To put data to same device\n",
    "        self.device = device\n",
    "        # Inferred word index for each time-step. For now, init with init time-step\n",
    "        self.word_indice = [torch.LongTensor(beam_size).zero_().to(self.device) + data_loader.BOS]\n",
    "        # Beam index for selected word index, at each time-step\n",
    "        self.beam_indice = [torch.LongTensor(beam_size).zero_().to(self.device)-1]\n",
    "        # cumulative log-probability for each beam\n",
    "        self.cumulative_probs = [torch.FloatTensor([.0]+[-float('inf')]*(beam_size-1)).to(self.device)]\n",
    "        # 1 if it is done else 0\n",
    "        self.masks = [torch.BoolTensor(Beam_size).zero_().to(self.device)]\n",
    "        \n",
    "        # we don't need to remember every time-step of hidden states\n",
    "        #        prev_hidden, prev_cell, prev_h_t_tilde\n",
    "        # what we need is remember just last one\n",
    "        self.prev_status = {}\n",
    "        self.batch_dims = {}\n",
    "        for prev_status_name, each_config in prev_status_config.items():\n",
    "            init_status = each_config['init_status']\n",
    "            batch_dim_index = each_config['batch_dim_index']\n",
    "            if init_status is not None:\n",
    "                self.prev_status[prev_status_name] = torch.cat([init_status]*beam_size,\n",
    "                                                              dim=batch_dim_index)\n",
    "            else:\n",
    "                self.prev_status[prev_status_name] = None\n",
    "            self.batch_dims[prev_status_name] = batch_dim_index\n",
    "            \n",
    "        self.current_time_step = 0\n",
    "        self.done_cnt = 0\n",
    "        \n",
    "    def get_length_penalty(self, \n",
    "                          length,\n",
    "                          alpha=LENGTH_PENALTY,\n",
    "                          min_length=MIN_LENGTH):\n",
    "        # calculate length-penalty\n",
    "        # because shorter sentence usually have bigger probabilty\n",
    "        # In fact, we represent this as log-probability, which is negative value\n",
    "        # Thus, we need to multiply bigger penalty for shorter one\n",
    "        p = ((min_length+1)/(min_length+length))**alpha\n",
    "        \n",
    "        return p\n",
    "    \n",
    "    def is_done(self):\n",
    "        # return 1, if we had EOS more than 'beam_size'-times\n",
    "        if self.done_cnt >= self.beam_size:\n",
    "            return 1\n",
    "        return 0\n",
    "    \n",
    "    def get_batch(self):\n",
    "        y_hat = self.word_indice[-1].unsqueeze(-1)\n",
    "        return y_hat, self.prev_status\n",
    "    \n",
    "    def collect_result(self, y_hat, prev_status):\n",
    "        output_size = y_hat.size(-1)\n",
    "        \n",
    "        self.current_time_step += 1\n",
    "    \n",
    "        cumulative_prob = self.cumulative_probs[-1].masked_fill_(self.masks[-1], -float('inf'))\n",
    "        cumulative_prob = y_hat + cumulative_prob.view(-1, 1, 1).expand(self.beam_size, 1, output_size)\n",
    "        # now, we have new top log-probability and its index\n",
    "        # we picked top index as many as 'beam_size'\n",
    "        # be aware that we picked top-k from whole batch through 'view(-1)'\n",
    "        \n",
    "        # following lines are using torch.sort, instead of using torch.topk\n",
    "        top_log_prob, top_indice = cumulative_prob.view(-1).sort(descending=True)\n",
    "        top_log_prob, top_indice = top_log_prob[:self.beam_size], top_indice[:self.beam_size]\n",
    "        \n",
    "        # because we picked from whole batch, original word index should be calculated again\n",
    "        self.word_indice += [top_indice.fmod(output_size)]\n",
    "        # also, we can get an index of beam, which has top-k log-probability search result\n",
    "        self.beam_indice += [top_indice.div(float(output_size)).long()]\n",
    "        \n",
    "        # add results to history boards\n",
    "        self.cumulative_probs += [top_log_prob]\n",
    "        self.masks += [torch.eq(self.word_indice[-1], data_loader.EOS)]  # set finish mask if we got EOS\n",
    "        # calculate a number of finished beams\n",
    "        self.done_cnt += self.masks[-1].float().sum()\n",
    "        \n",
    "        # In beam search procedure, we only need to memorize latest status\n",
    "        # for seq2seq, it would be latest hidden and cell state, and h_t_tilde\n",
    "        # The problem is hidden(or cell) state and h_t_tilde has different dimension order\n",
    "        \n",
    "        # In other words, a dimension for batch index is different\n",
    "        # For transformer, latest status is each layer's decoder output from the beginning\n",
    "        # Unlike seq2seq, transformer has to memorize every previous output for attention operation\n",
    "        for prev_status_name, prev_status in prev_status.items():\n",
    "            self.prev_status[prev_status_name] = torch.index_select(\n",
    "                                                 prev_status,\n",
    "                                                dim=self.batch_dims[prev_status_name],\n",
    "                                                index=self.beam_indice[-1]).contiguous()\n",
    "            \n",
    "    def get_n_best(self, n=1, length_penalty=.2):\n",
    "        sentences, probs, founds = [], [], []\n",
    "        \n",
    "        for t in range(len(self.word_indce)):    # for each time-step\n",
    "            for b in range(self.beam_size):     # for each beam\n",
    "                if self.masks[t][b] == 1:  # if we had EOS on this time-step and beam\n",
    "                    # take a record of penaltified log-probability\n",
    "                    probs += [self.cumulative_probs[t][b] *\\\n",
    "                                     self.get_length_penalty(t, alpha=length_penalty)]\n",
    "                    founds += [(t,b)]\n",
    "                    \n",
    "        sorted_founds_with_probs = sorted(\n",
    "                                    zip(founds, probs),\n",
    "                                    key=itemgetter(1),\n",
    "                                    reverse=True)[:n]\n",
    "        probs = []\n",
    "        \n",
    "        for (end_index, b), prob in sorted_founds_with_probs:\n",
    "            sentence = []\n",
    "            \n",
    "            # trace from the end\n",
    "            for t in range(end_index, 0, -1):\n",
    "                sentence = [self.word_indice[t][b]] + sentence\n",
    "                b = self.beam_indice[t][b]\n",
    "                \n",
    "            sentences += [sentence]\n",
    "            probs += [prob]\n",
    "            \n",
    "        return sentences, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013962,
     "end_time": "2021-02-24T08:22:14.695909",
     "exception": false,
     "start_time": "2021-02-24T08:22:14.681947",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-24T08:22:14.740501Z",
     "iopub.status.busy": "2021-02-24T08:22:14.739805Z",
     "iopub.status.idle": "2021-02-24T08:22:14.743296Z",
     "shell.execute_reply": "2021-02-24T08:22:14.742768Z"
    },
    "papermill": {
     "duration": 0.033292,
     "end_time": "2021-02-24T08:22:14.743436",
     "exception": false,
     "start_time": "2021-02-24T08:22:14.710144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PAD,BOS,EOS = 1,2,3\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self,\n",
    "                train_fn=None,\n",
    "                valid_fn=None,\n",
    "                exts=None,\n",
    "                batch_size=64,\n",
    "                device='cpu',\n",
    "                max_vocab=999999999,\n",
    "                max_length=255,\n",
    "                fix_length=None,\n",
    "                use_bos=True,\n",
    "                use_eos=True,\n",
    "                shuffle=True,\n",
    "                dsl=False):\n",
    "        \n",
    "        super(DataLoader, self).__init__()\n",
    "        \n",
    "        self.src = data.Field(\n",
    "                    sequential=True,\n",
    "                    use_vocab=True,\n",
    "                    batch_first=True,\n",
    "                    include_lengths=True,\n",
    "                    fix_length=fix_length,\n",
    "                    init_token=\"<BOS>\" if dsl else None,\n",
    "                    eos_token=\"<EOS>\" if dsl else None)\n",
    "        \n",
    "        self.tgt = data.Field(\n",
    "                    sequential=True,\n",
    "                    use_vocab=True,\n",
    "                    batch_first=True,\n",
    "                    include_lengths=True,\n",
    "                    fix_length=fix_length,\n",
    "                    init_token=\"<BOS>\" if use_bos else None,\n",
    "                    eos_token=\"<EOS>\" if use_eos else None)\n",
    "        \n",
    "        if train_fn is not None and valid_fn is not None and exts is not None:\n",
    "            train = TranslationDataset(\n",
    "                path=train_fn,\n",
    "                exts=exts,\n",
    "                fields=[('src',self.src),('tgt',self.tgt)],\n",
    "                max_length=max_length)\n",
    "            \n",
    "            valid = TranslationDataset(\n",
    "                path=valid_fn,\n",
    "                exts=exts,\n",
    "                fields=[(\"src\",self.src),(\"tgt\",self.tgt)],\n",
    "                max_length=max_length)\n",
    "            \n",
    "            self.train_iter = data.BucketIterator(\n",
    "                train,\n",
    "                batch_size=batch_size,\n",
    "                device=\"cuda\" if cuda.is_available() else 'cpu',\n",
    "                shuffle=shuffle,\n",
    "                sort_key=lambda x:len(x.tgt)+(max_length*len(x.src)),\n",
    "                sort_within_batch=True)\n",
    "            \n",
    "            self.valid_iter = data.BucketIterator(\n",
    "                valid,\n",
    "                batch_size=batch_size,\n",
    "                device='cuda' if cuda.is_available() else 'cpu',\n",
    "                shuffle=False,\n",
    "                sort_key=lambda x:len(x.tgt)+(max_length*len(x.src)),\n",
    "                sort_within_batch=True)\n",
    "            \n",
    "            self.src.build_vocab(train, max_size=max_vocab)\n",
    "            self.tgt.build_vocab(train, max_size=max_vocab)\n",
    "            \n",
    "        \n",
    "        def load_vocab(self, src_vocab, tgt_vocab):\n",
    "            self.src.vocab = src_vocab\n",
    "            self.tgt.vocab = tgt_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-24T08:22:14.784613Z",
     "iopub.status.busy": "2021-02-24T08:22:14.783604Z",
     "iopub.status.idle": "2021-02-24T08:22:14.786927Z",
     "shell.execute_reply": "2021-02-24T08:22:14.786413Z"
    },
    "papermill": {
     "duration": 0.029153,
     "end_time": "2021-02-24T08:22:14.787075",
     "exception": false,
     "start_time": "2021-02-24T08:22:14.757922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TranslationDataset(data.Dataset):\n",
    "    \"\"\"Defines a dataset for machine translation\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return data.interleave_keys(len(ex.src),len(ex.trg))\n",
    "    \n",
    "    def __init__(self, path, exts, fields, max_length=None, **kwards):\n",
    "        if not isinstance(fields[0], (tuple, list)):\n",
    "            fields = [(\"src\",fields[0]),(\"trg\",fields[1])]\n",
    "            \n",
    "        if not path.endswith(\".\"):\n",
    "            path += \".\"\n",
    "            \n",
    "        src_path, trg_path = tuple(os.path.expanduser(path+x) for x in exts)\n",
    "        examples = []\n",
    "        \n",
    "        with open(src_path, encoding='uf-8') as src_file, open(trg_path,encoding='utf-8') as trg_file:\n",
    "            for src_line, trg_line in zip(src_file, trg_file):\n",
    "                src_line, trg_line = src_line.strip(), trg_line.strip()\n",
    "                if max_length and max_length < max(len(src_line.split()), len(trg_line.split())):\n",
    "                    continue\n",
    "                if src_line != '' and trg_line != '':\n",
    "                    examples += [data.Example.fromlist([src_line,trg_line], fields)]\n",
    "                    \n",
    "        super().__init__(examples, fields, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014034,
     "end_time": "2021-02-24T08:22:14.815729",
     "exception": false,
     "start_time": "2021-02-24T08:22:14.801695",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-24T08:22:14.860032Z",
     "iopub.status.busy": "2021-02-24T08:22:14.859171Z",
     "iopub.status.idle": "2021-02-24T08:22:14.861700Z",
     "shell.execute_reply": "2021-02-24T08:22:14.862139Z"
    },
    "papermill": {
     "duration": 0.032177,
     "end_time": "2021-02-24T08:22:14.862322",
     "exception": false,
     "start_time": "2021-02-24T08:22:14.830145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_grad_norm(parameters, norm_type=2):\n",
    "    parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
    "    total_norm = 0\n",
    "    \n",
    "    try:\n",
    "        for p in parameters:\n",
    "            total_norm += (p.grad.data ** norm_type).sum()\n",
    "        total_norm = total_norm ** (1. / norm_type)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    return total_norm\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_paramter_norm(parameters, norm_type=2):\n",
    "    total_norm = 0\n",
    "    \n",
    "    try:\n",
    "        for p in parameters:\n",
    "            total_norm += (p.data ** norm_type).sum()\n",
    "        total_norm = total_norm ** (1. / norm_type)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    return total_norm\n",
    "\n",
    "\n",
    "def sort_by_length(x, lengths):\n",
    "    batch_size = x.size(0)\n",
    "    x = [x[i] for i in range(batch_size)]\n",
    "    lengths = [lengths[i] for i in range(batch_size)]\n",
    "    orders = [i for i in range(batch_size)]\n",
    "    \n",
    "    sorted_outputs = sorted(zip(x, lengths, orders), key=itemgetter(1), reverse=True)\n",
    "    sorted_x = torch.stack([sorted_tuples[i][0] for i in range(batch_size)])\n",
    "    sorted_lengths = torch.stack(sorted_tuples[i][1] for i in range(batch_size))\n",
    "    sorted_orders = [sorted_tuples[i][2] for i in range(batch_size)]\n",
    "    \n",
    "    return sorted_x, sorted_lengths, sorted_orders\n",
    "\n",
    "\n",
    "\n",
    "def sort_by_order(x, orders):\n",
    "    batch_size = x.size(0)\n",
    "    x = [x[i] for i in range(batch_size)]\n",
    "    sorted_tuples = sorted(zip(x, orders), key=itemgetter(1))\n",
    "    sorted_x = torch.stack([sorted_tuples[i][0] for i in range(batch_size)])\n",
    "    \n",
    "    return sorted_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014465,
     "end_time": "2021-02-24T08:22:14.892918",
     "exception": false,
     "start_time": "2021-02-24T08:22:14.878453",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-24T08:22:14.930786Z",
     "iopub.status.busy": "2021-02-24T08:22:14.930008Z",
     "iopub.status.idle": "2021-02-24T08:22:14.933112Z",
     "shell.execute_reply": "2021-02-24T08:22:14.932588Z"
    },
    "papermill": {
     "duration": 0.025643,
     "end_time": "2021-02-24T08:22:14.933251",
     "exception": false,
     "start_time": "2021-02-24T08:22:14.907608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.linear = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, h_src, h_t_tgt, mask=None):      \n",
    "        query = self.linear(h_t_tgt)\n",
    "        weight = torch.bmm(query, h_src.transpose(1,2))\n",
    "        \n",
    "        if mask is not None:\n",
    "            # set each weight as -inf, if the mask value equals to 1.\n",
    "            # Since the softmax operation makes -inf to 0,\n",
    "            # masked weights would be set to 0 after softmax operation.\n",
    "            # Thus, if the sample is shorter than other samples in mini-batch,\n",
    "            # the weight for empty time-step would be set to 0.\n",
    "            weight.masked_fill_(mask.unsqueeze(1), -float('inf'))\n",
    "        \n",
    "        weight = self.softmax(weight)\n",
    "        context_vector = torch.bmm(weight, h_src)\n",
    "        \n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-24T08:22:14.971911Z",
     "iopub.status.busy": "2021-02-24T08:22:14.971158Z",
     "iopub.status.idle": "2021-02-24T08:22:14.974500Z",
     "shell.execute_reply": "2021-02-24T08:22:14.973955Z"
    },
    "papermill": {
     "duration": 0.026565,
     "end_time": "2021-02-24T08:22:14.974644",
     "exception": false,
     "start_time": "2021-02-24T08:22:14.948079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, word_vec_size, hidden_size, n_layers=4, dropout_p=.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Be aware of value of 'batch_first' parameter\n",
    "        # Also, its hidden_size is half of original hidden_size,\n",
    "        # because it is bidirectional ###TODO: 웨??????\n",
    "        \n",
    "        self.rnn = nn.LSTM(word_vec_size,\n",
    "                          int(hidden_size/2),\n",
    "                          num_layers = n_layers,\n",
    "                          dropout=dropout_p,\n",
    "                          bidirectional=True,\n",
    "                          batch_first=True)\n",
    "        \n",
    "    def forward(self, emb):\n",
    "        # |emb| = (batch_size, length, word_vec_size)\n",
    "        if isinstance(emb, tuple):\n",
    "            x, lengths = emb\n",
    "            x = pack(x, lengths.tolist(), batch_first=True)\n",
    "            \n",
    "            # Below is how pack_padded_sequence works.\n",
    "            # as you can see,\n",
    "            # PackedSequence object has information about mini-batch-wise information,\n",
    "            # not time-step-wise information.\n",
    "        else:\n",
    "            x = emb\n",
    "            \n",
    "        y, h = self.rnn(x)\n",
    "        if isinstance(emb, tuple):\n",
    "            y, _ = unpack(y, batch_first=True)\n",
    "            \n",
    "        return y, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-24T08:22:15.014028Z",
     "iopub.status.busy": "2021-02-24T08:22:15.013317Z",
     "iopub.status.idle": "2021-02-24T08:22:15.016035Z",
     "shell.execute_reply": "2021-02-24T08:22:15.016531Z"
    },
    "papermill": {
     "duration": 0.026914,
     "end_time": "2021-02-24T08:22:15.016704",
     "exception": false,
     "start_time": "2021-02-24T08:22:14.989790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, word_vec_size, hidden_size, n_layers=4, dropout_p=.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Be aware of value of 'batch_first' parameter and 'bidirectional' parameter\n",
    "        self.rnn = nn.LSTM(\n",
    "                word_vec_size + hidden_size,\n",
    "                hidden_size,\n",
    "                num_layers=n_layers,\n",
    "                dropout=dropout_p,\n",
    "                bidirectional=False,\n",
    "                batch_first=True)\n",
    "        \n",
    "        \n",
    "    def forward(self, emb_t, h_t_1_tilde, h_t_1):\n",
    "        batch_size = emb_t.size(0)\n",
    "        hidden_size = h_t_1[0].size(-1)\n",
    "        \n",
    "        if h_t_1_tilde is None:\n",
    "            # if this is the first time-step\n",
    "            h_t_1_tilde = emb_t.new(batch_size, 1, hidden_size).zero_()\n",
    "            \n",
    "        # input feeding trick\n",
    "        x = torch.cat([emb_t, h_t_1_tilde], dim=-1)\n",
    "        \n",
    "        # unlike encoder, decoder must take an input for sequentially\n",
    "        y, h = self.rnn(x, h_t_1)\n",
    "        return y, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-24T08:22:15.053939Z",
     "iopub.status.busy": "2021-02-24T08:22:15.053292Z",
     "iopub.status.idle": "2021-02-24T08:22:15.056510Z",
     "shell.execute_reply": "2021-02-24T08:22:15.055875Z"
    },
    "papermill": {
     "duration": 0.024804,
     "end_time": "2021-02-24T08:22:15.056651",
     "exception": false,
     "start_time": "2021-02-24T08:22:15.031847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):      \n",
    "        y = self.softmax(self.output(x))\n",
    "        \n",
    "        # return log-probability instead of just probability\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-24T08:22:15.132951Z",
     "iopub.status.busy": "2021-02-24T08:22:15.096634Z",
     "iopub.status.idle": "2021-02-24T08:22:15.142191Z",
     "shell.execute_reply": "2021-02-24T08:22:15.141640Z"
    },
    "papermill": {
     "duration": 0.070304,
     "end_time": "2021-02-24T08:22:15.142333",
     "exception": false,
     "start_time": "2021-02-24T08:22:15.072029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_size, word_vec_size, hidden_size, output_size,\n",
    "                 n_layers=4, dropout_p=.2) :\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.word_vec_size = word_vec_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.emb_src = nn.Embedding(input_size, word_vec_size)\n",
    "        self.emb_dec = nn.Embedding(output_size, word_vec_size)\n",
    "        \n",
    "        self.encoder = Encoder(word_vec_size, hidden_size,\n",
    "                              n_layers=n_layers, dropout_p=dropout_p)\n",
    "        \n",
    "        self.decoder = Decoder(word_vec_size, hidden_size,\n",
    "                              n_layers=n_layers, dropout_p=dropout_p)\n",
    "        \n",
    "        self.attn = Attention(hidden_size)\n",
    "        \n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.generator = Generator(hidden_size, output_size)\n",
    "        \n",
    "    def generate_mask(self, x, length):\n",
    "        mask = []\n",
    "        \n",
    "        max_length = max(length)\n",
    "        for l in length:\n",
    "            if max_length - l > 0:\n",
    "                # if the length is shorter than maximum length among samples,\n",
    "                # set last few values to be 1s to remove attention weight.\n",
    "                mask += [torch.cat([x.new_ones(1,l).zero_(),\n",
    "                                   x.new_ones(1, (max_length-l))],\n",
    "                                  dim=-1)]\n",
    "                \n",
    "            else:\n",
    "                # if the length of the sample equals to maximum length among samples,\n",
    "                # set every value in mask to be 0.\n",
    "                mask += [x.new_ones(1,l).zero_()]\n",
    "                \n",
    "        mask = torch.cat(mask, dim=0).bool()\n",
    "        return mask\n",
    "    \n",
    "    \n",
    "    def merge_encoder_hiddens(self, encoder_hiddens):\n",
    "        new_hiddens = []\n",
    "        new_cells = []\n",
    "        \n",
    "        hiddens, cells = encoder_hiddens\n",
    "        \n",
    "        # i-th and (i+l)-th layer is opposite direction.\n",
    "        # also, each direction of layer is half hidden size\n",
    "        # therefore, we concatenate both directions to 1 hidden size layer\n",
    "        for i in range(0, hiddens.size(0), 2):\n",
    "            new_hiddens += [torch.cat([hiddens[i], hiddens[i+1]], dim=-1)]\n",
    "            new_cells += [torch.cat([cells[i], cells[i+1]], dim=-1)]\n",
    "            \n",
    "        new_hiddens, new_cells = torch.stack(new_hiddens), torck.stack(new_cells)\n",
    "        return (new_hiddens, new_cells)\n",
    "    \n",
    "    def fast_merge_encoder_hiddens(self, encoder_hiddens):\n",
    "        # merge bidirectional to uni-directional\n",
    "        # we need to convert size from (n_layers*2, batch_size, hidden_size/2)\n",
    "        # to (n_layers, batch_size, hidden_size)\n",
    "        # Thus, the converting operation will not working with just 'view' method\n",
    "        h_0_tgt, c_0_tgt = encoder_hiddens\n",
    "        batch_size = h_0_tgt.size(1)\n",
    "        \n",
    "        h_0_tgt = h_0_tgt.transpose(0,1).contiguous().view(batch_size,\n",
    "                                                          -1,\n",
    "                                                self.hidden_size).transpose(0,1).contiguous()\n",
    "        c_0_tgt = c_0_tgt.transpose(0,1).contiguous().view(batch_size,\n",
    "                                                          -1,\n",
    "                                                          self.hidden_size).transpose(0,1).contiguous()\n",
    "        # you can use 'merge_encoder_hiddens' method, instead of using abovec 3 lines\n",
    "        # 'merge_encoder_hiddens' method works with non-parallel way\n",
    "        \n",
    "        return h_0_tgt, c_0_tgt\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        batch_size = tgt.size(0)\n",
    "        \n",
    "        mask = None\n",
    "        x_length = None\n",
    "        if isinstance(src, tuple):\n",
    "            x, x_length = src\n",
    "            # based on the length information, generate mask to prevent that\n",
    "            # shorter sample has wasted attention\n",
    "            mask = self.generate_mask(x, x_length)\n",
    "        else:\n",
    "            x = src\n",
    "            \n",
    "        if isinstance(tgt, tuple):\n",
    "            tgt = tgt[0]\n",
    "            \n",
    "        # get word embedding vectors for every time-step of input sentence\n",
    "        emb_src = self.emb_src(x)\n",
    "    \n",
    "        # the last hidden state of the encoder would be a initial hidden state of decoder\n",
    "        h_src, h_0_tgt = self.encoder((emb_src, x_length))\n",
    "    \n",
    "        h_0_tgt = self.fast_merge_encoder_hiddens(h_0_tgt)\n",
    "        emb_tgt = self.emb_dec(tgt)\n",
    "\n",
    "        h_tilde = []\n",
    "        \n",
    "        h_t_tilde = None\n",
    "        decoder_hidden = h_0_tgt\n",
    "        # run decoder until the end of the time-step\n",
    "        for t in range(tgt.size(1)):\n",
    "            # Teacher forcing: take each input from training set\n",
    "            # not from the last time-step's output\n",
    "            # because of Teacher forcing\n",
    "            # training procedure and inference procedure becomes different\n",
    "            # of course, because of sequential running in decoder,\n",
    "            # this causes servere bottle-neck\n",
    "            emb_t = emb_tgt[:, t, :].unsqueeze(1)\n",
    "            \n",
    "            decoder_output, decoder_hidden = self.decoder(emb_t, \n",
    "                                                         h_t_tilde,\n",
    "                                                         decoder_hidden)\n",
    "\n",
    "            context_vector = self.attn(h_src, decoder_output, mask)\n",
    "            h_t_tilde = self.tanh(self.concat(torch.cat([decoder_output,\n",
    "                                                        context_vector],\n",
    "                                                       dim=-1)))\n",
    "\n",
    "            h_tilde += [h_t_tilde]\n",
    "        \n",
    "        h_tilde = torch.cat(h_tilde, dim=1)\n",
    "        y_hat = self.generator(h_tilde)\n",
    "        return y_hat\n",
    "    \n",
    "    \n",
    "    def search(self, src, is_greedy=True, max_length=255):\n",
    "        if isinstance(src, tuple):\n",
    "            x, x_length = src\n",
    "            mask = self.generate_mask(x, x_length)\n",
    "        else:\n",
    "            x, x_length = src, None\n",
    "            mask = None\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # same procedure as teacher forcicng\n",
    "        emb_src = self.emb_src(x)\n",
    "        h_src, h_0_tgt = self.encoder((emb_src, x_length))\n",
    "        decoder_hidden = self.fast_mergge_encoder_hiddens(h_0_tgt)\n",
    "        \n",
    "        # fill a vetor, which has 'batch_size' dimension, with BOS value\n",
    "        y = x.new(batch_-size, 1).zero_() + data_loader.BOS\n",
    "        \n",
    "        is_decoding = x.new_ones(batch_size, 1).bool()\n",
    "        h_t_tilde, y_hats, indice = None, [], []\n",
    "        \n",
    "        # repeat a loop while sum of 'is_decoding' flag is bigger than 0\n",
    "        # or current time-step is smaller than maximum length\n",
    "        while is_decoding.sum() > 0 and len(indice) < max_length:\n",
    "            # Unlike training procedure,\n",
    "            # take the last time-step's output during the inference\n",
    "            emb_t = self.emb_dec(y)\n",
    "            \n",
    "            decoder_output, decoder_hidden = self.decoder(emb_t,\n",
    "                                                         h_t_tilde,\n",
    "                                                         decoder_hidden)\n",
    "            context_vector = self.attn(h_src, decoder_output, mask)\n",
    "            h_t_tilde = self.tanh(self.concat(torch.cat([decoder_output,\n",
    "                                                        context_vector],\n",
    "                                                       dim=-1)))\n",
    "            y_hat = self.generator(h_t_tilde)\n",
    "            y_hats += [y_hat]\n",
    "            \n",
    "            if is_greedy:\n",
    "                y = y_hat.argmax(dim=-1)\n",
    "            else:\n",
    "                # Take a random sampling based on the multinoulli dist\n",
    "                y = torch.multinomial(y_hat.exp().view(batch_size, -1),  1)\n",
    "            \n",
    "            # put PAD if the sample is done\n",
    "            y = y.masked_fill_(~is_decoding, data_loader.PAD)\n",
    "            # Update is_decoding if there is EOS token\n",
    "            is_decoding = is_decoding * torch.ne(y, data_loader.EOS)\n",
    "\n",
    "            indice += [y]\n",
    "            \n",
    "        y_hats = torch.cat(y_hats, dim=1)\n",
    "        indice = torch.cat(indice, dim=1)\n",
    "        return y_hats, indice\n",
    "    \n",
    "\n",
    "    def batch_beam_search(self,\n",
    "                         src,\n",
    "                         beam_size=5,\n",
    "                         max_length=255,\n",
    "                         n_best=1,\n",
    "                         length_penalty=.2):\n",
    "\n",
    "        mask, x_length = None, None\n",
    "        if isinstance(src, tuple):\n",
    "            x, x_length = src\n",
    "            mask = self.generate_mask(x, x_length)\n",
    "        else:\n",
    "            x = src\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        emb_src = self.emb_src(x)\n",
    "        h_src, h_0_tgt = self.encoder((emb_src, x_length))\n",
    "        h_0_tgt = self.fast_merge_encoder(hiddens(h_0_tgt))\n",
    "        \n",
    "        # initialize 'SingleBeamSearchBoard' as many as batch_size\n",
    "        boards = [SingleBeamSearchBoard(\n",
    "                    h_src.device,\n",
    "                    {\n",
    "                        'hidden_state':{\n",
    "                            'init_status':h_0_tgt[0][:, i, :].unsqueeze(1),\n",
    "                            'batch_dim_index':1, \n",
    "                        },    # |hidden_state| = (n_layers, batch_size, hidden_size)\n",
    "                        'cell_state':{\n",
    "                            'init_status':h_0_tgt[1][:, i, :].unsqueeze(1),\n",
    "                            'batch_dim_index':1,\n",
    "                        },   # |cell_state| = (n_layers, batch_size, hidden_size)\n",
    "                        'h_t_1_tilde':{\n",
    "                            'init_status':None,\n",
    "                            'batch_dim_index':0,\n",
    "                        },   # |h_t_1_tilde| = (batch_size, 1, hidden_size)\n",
    "                    },\n",
    "            beam_size=beam_size, max_length=max_length) for i in range(batch_size)]\n",
    "        is_done = [board.is_done() for board in boards]\n",
    "        \n",
    "        length = 0\n",
    "        # Run loop while sum of 'is_done' is smaller than batch_size,\n",
    "        # or length is still smaller than max_length\n",
    "        while sum(is_done) < batch_size and length <= max_length:\n",
    "            # current_batch_size = sum(is_done) * beam_size\n",
    "            \n",
    "            # initialize fabricated variables\n",
    "            # As far as batch-beam-search is running\n",
    "            # temporary batch-size for fabricate mini-batch is\n",
    "            # 'beam_size'-times bigger than original batch_size\n",
    "            fab_input, fab_hidden, fab_cell, fab_h_t_tilde = [], [], [], []\n",
    "            fab_h_src, fab_mask = [], []\n",
    "            \n",
    "            # Build fabricated mini-batch in non-parallel way\n",
    "            # this may cause a bottle-neck\n",
    "            for i, board in enumerate(boards):\n",
    "                # Batchify if the inference for the sample is still not finished\n",
    "                if board.is_done() == 0:\n",
    "                    y_hat_i, prev_status = board.get_batch()\n",
    "                    hidden_i = prev_status['hidden_state']\n",
    "                    cell_i = prev_status['cell_state']\n",
    "                    h_t_tilde_i = prev_status['h_t_1_tilde']\n",
    "                    \n",
    "                    fab_input += [y_hat_i]\n",
    "                    fab_hidden += [hidden_i]\n",
    "                    fab_cell += [cell_i]\n",
    "                    fab_h_src += [h_src[i, :, :]] * beam_size\n",
    "                    fab_mask += [mask[i, :]] * beam_size\n",
    "                    if h_t_tilde_i is not None:\n",
    "                        fab_h_t_tilde += [h_t_tilde_i]\n",
    "                    else:\n",
    "                        fab_h_t_tilde = None\n",
    "                        \n",
    "            # Now, concatenate list of tensors\n",
    "            fab_input = torch.cat(fab_input, dim=0)\n",
    "            fab_hidden = torch.cat(fab_hidden, dim=1)\n",
    "            fab_cell = torch.cat(fab_cell, dim=1)\n",
    "            fab_h_src = torch.stack(fab_h_src)\n",
    "            fab_mask = torch.stack(fab_mask)\n",
    "            if fab_h_t_tilde is not None:\n",
    "                fab_h_t_tilde = torch.cat(fab_h_t_tilde, dim=0)\n",
    "            \n",
    "            emb_t = self.emb_dec(fab_input)  \n",
    "            fab_decoder_output, (fab_hidden, fab_cell) = self.decoder(emb_t, \n",
    "                                                                     fab_h_t_tilde,\n",
    "                                                                     (fab_hidden, fab_cell))\n",
    "            context_vector = self.attn(fab_h_src, fab_decoder_output, fab_mask)\n",
    "            \n",
    "            fab_h_t_tilde = self.tanh(self.concat(torch.cat([fab_decoder_output,\n",
    "                                                            context_vector], \n",
    "                                                           dim=-1)))\n",
    "            y_hat = self.generator(fab_h_t_tilde)\n",
    "            \n",
    "            cnt = 0\n",
    "            for board in boards:\n",
    "                if board.is_done() == 0:\n",
    "                    # decide a range of each sample\n",
    "                    begin = cnt * beam_size\n",
    "                    end = begin + beam_size\n",
    "                    \n",
    "                    # pick k-best results for each sample\n",
    "                    board.collect_result(\n",
    "                    y_hat[begin:end],\n",
    "                    {\n",
    "                        'hidden_state': fab_hidden[:, begin:end, :],\n",
    "                        'cell_state': fab_cell[:, begin:end, :],\n",
    "                        'h_t_1_tilde': fab_h_t_tilde[begin:end],\n",
    "                    })\n",
    "                    cnt += 1\n",
    "\n",
    "            is_done = [board.is_done() for board in boards]\n",
    "            length += 1\n",
    "            \n",
    "        # pick n-best hypothesis\n",
    "        batch_sentences, batch_probs = [], []\n",
    "        \n",
    "        # collect the results\n",
    "        for i, board in enumerate(boards):\n",
    "            sentences, probs = board.get_n_best(n_best, length_penalty=length_penalty)\n",
    "            \n",
    "            batch_sentences += [sentences]\n",
    "            batch_probs += [probs]\n",
    "        \n",
    "        return batch_sentences, batch_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.0151,
     "end_time": "2021-02-24T08:22:15.172787",
     "exception": false,
     "start_time": "2021-02-24T08:22:15.157687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.014982,
     "end_time": "2021-02-24T08:22:15.203134",
     "exception": false,
     "start_time": "2021-02-24T08:22:15.188152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.015109,
     "end_time": "2021-02-24T08:22:15.233824",
     "exception": false,
     "start_time": "2021-02-24T08:22:15.218715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8.55617,
   "end_time": "2021-02-24T08:22:15.960068",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-02-24T08:22:07.403898",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
