{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPU 사용하는 법\n",
    "1) https://pytorch.org/xla/release/1.7/index.html<br>\n",
    "2) https://pytorch.org/xla/release/1.7/index.html#xla-tensor-deep-dive<br><br><br>\n",
    "### torchtext 사용하는 법\n",
    "1) https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb<br><br>\n",
    "#### 1) 이 코드를 전부 돌려보고 정상적으로 돌아가는지 확인한다 (stopwords 처리 안함)<br>\n",
    "#### 2) stopwords를 삭제하고 나서, 1)번과 성능 차이를 본다<br>\n",
    "#### 3) torchtext 버전으로 코드를 수정해보고, 또 stopwords 삭제도 해보면서 성능 차이를 확인한다.<br><br><br>\n",
    "### References<br>\n",
    "1) https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpu = False\n",
    "if tpu:\n",
    "    !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "    !python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\n",
    "    \n",
    "    import torch_xla\n",
    "    import torch_xla.core.xla_model as xm\n",
    "\n",
    "# https://www.kaggle.com/tanlikesmath/the-ultimate-pytorch-tpu-tutorial-jigsaw-xlm-r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import os, re, gensim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "if tpu:\n",
    "    os.environ['XLA_USE_BF16'] = '1'\n",
    "    os.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '100000000'\n",
    "    device = xm.xla_device()\n",
    "\n",
    "### stopwords 제거없이도 충분히 좋은 성능\n",
    "# from nltk.corpus import stopwords\n",
    "# stop = stopwords.words('english')\n",
    "from collections import defaultdict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## modified codes in https://github.com/yoonkim/CNN_sentence \n",
    "class preprocess:\n",
    "    def __init__(self, word_vecs, save=False):\n",
    "        self.k = 300     # the embedding dimension of pretrained vector noted at the paper\n",
    "        self.revs = []\n",
    "        self.vocab_size = 0\n",
    "        self.max_len = 56 # the value what yoon used at his codes\n",
    "        self.word_idx_map = dict()\n",
    "        self.word_vecs = word_vecs\n",
    "        self.stop = set(stopwords.words('english'))\n",
    "        self.save = save\n",
    "\n",
    "        \n",
    "    def clean_str(self, string):\n",
    "        ## string이 sentence로 들어와서, self.stop으로 못 거름...=_=;\n",
    "        ## 우선 stopwords 안 거르는걸로 해서 함 돌려보자.\n",
    "        string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "        string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "        string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "        string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "        string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "        string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "        string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "        string = re.sub(r\",\", \" , \", string) \n",
    "        string = re.sub(r\"!\", \" ! \", string) \n",
    "        string = re.sub(r\"\\(\", \" \\( \", string) \n",
    "        string = re.sub(r\"\\)\", \" \\) \", string) \n",
    "        string = re.sub(r\"\\?\", \" \\? \", string) \n",
    "        string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "        return string.strip().lower()\n",
    "    \n",
    "    \n",
    "    def build_data_cv(self, cv=10):\n",
    "        pos_file = \"/kaggle/input/posneg-for-cnn4sc/rt-polarity.pos\"\n",
    "        neg_file = \"/kaggle/input/posneg-for-cnn4sc/rt-polarity.neg\"\n",
    "\n",
    "        file_list = [pos_file, neg_file]    \n",
    "        self.vocab = defaultdict(float)\n",
    "\n",
    "        for file in file_list:\n",
    "            with open(file, \"rb\") as f:\n",
    "                for line in f: \n",
    "                    try:\n",
    "                        line = line.decode(\"utf-8\")\n",
    "                    except UnicodeDecodeError:\n",
    "                        line = line.decode('latin-1')\n",
    "\n",
    "                    rev = []\n",
    "                    rev.append(line.strip())\n",
    "                    orig_rev = self.clean_str(\" \".join(rev))\n",
    "        \n",
    "                    words = set(orig_rev.split())\n",
    "                    \n",
    "                    if len(words) > self.max_len:\n",
    "                        self.max_len = len(words)\n",
    "                    \n",
    "                    for word in words:\n",
    "                        try:\n",
    "                            self.vocab[word] += 1\n",
    "                            self.vocab_size += 1\n",
    "                        except:\n",
    "                            self.vocab[word] = 1\n",
    "                            self.vocab_size += 1\n",
    "                            \n",
    "                    if file[-3:] == \"pos\":        \n",
    "                        datum = {\"y\": 1,\n",
    "                                \"text\": orig_rev,\n",
    "                                \"split\": np.random.randint(0,cv)}\n",
    "                        ## np.random.randint는 discrete uniform distribution\n",
    "                        \n",
    "                    elif file[-3:] == \"neg\":\n",
    "                        datum = {\"y\": 0,\n",
    "                                \"text\": orig_rev,\n",
    "                                \"split\": np.random.randint(0,cv)}\n",
    "                    self.revs.append(datum)\n",
    "        \n",
    "        return self.revs, self.vocab, self.max_len\n",
    "    # self.revs :  sentence와 label 데이터\n",
    "    # self.vocab : vocabulary set with its frequency\n",
    "    # self.word_vecs : google pre-trained word vector\n",
    "    ### 단어는 없고 word_idx와 word_vector만 갖고 있음\n",
    "    \n",
    "    # self.\n",
    "    \n",
    "    def add_unknown_words(self, min_df=1):\n",
    "        cnt = 0\n",
    "        for word in self.vocab:\n",
    "            if word not in self.word_vecs.keys() and self.vocab[word] >= min_df:\n",
    "                self.word_vecs[word] = np.random.uniform(-0.25,0.25,self.k)  \n",
    "                self.vocab_size += 1\n",
    "                cnt += 1\n",
    "        print(cnt, ' of unknown words were here')          \n",
    "\n",
    "    def get_W(self):\n",
    "        self.revs, self.vocab, self.max_len = self.build_data_cv()\n",
    "        self.add_unknown_words()\n",
    "        self.W = np.zeros(shape=(self.vocab_size+1, self.k), dtype='float32')            \n",
    "#         self.W[0] = np.zeros(self.k, dtype='float32')\n",
    "        # 왜 굳이 W의 0번 인덱스를 따로 해주고, i는 1부터 시작하게 해놨지?\n",
    "        i = 0\n",
    "        for word in self.vocab:\n",
    "            print(i, 'th word:', word)\n",
    "            self.W[i] = self.word_vecs[word]\n",
    "            self.word_idx_map[word] = i\n",
    "            i += 1\n",
    "            \n",
    "        if self.save:\n",
    "            self.save_file()\n",
    "#         print('W shape:', self.W.shape)\n",
    "        # word_idx_map은 dictionary 타입이다.\n",
    "        return self.W, self.word_idx_map, self.revs, self.max_len\n",
    "    \n",
    "    def save_file(self):\n",
    "        data = {'revs':self.revs,\n",
    "               'w':self.W,\n",
    "               'word_idx_map':self.word_idx_map,\n",
    "               'vocab':self.vocab}\n",
    "        \n",
    "        pd.Series(data).to_json('data.json')\n",
    "        print('making a json file completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## when the \"data.json\" file doesn't exist \n",
    "# pre = preprocess(word_vecs, save=True)\n",
    "# w, word_idx_map, revs, max_len = pre.get_W()\n",
    "\n",
    "## if \"data.json\" exist\n",
    "with open(\"/kaggle/input/cnn-word-vector-json/data.json\") as json_file:\n",
    "    files = json.load(json_file)\n",
    "    \n",
    "# w =  files['w']  # 자체 embedding 들어가서 w 만들 필요 없음\n",
    "revs, word_idx_map, vocab = files['revs'], files['word_idx_map'], files['vocab']\n",
    "## word_vecs : dictionary형, 벡터를 보고프면 word_vecs['word']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build each Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Make_Dataset(TensorDataset):\n",
    "    def __init__(self, word_idx_map, xy):\n",
    "        '''\n",
    "        self.xy : a sentence, string type\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.xy = xy\n",
    "        self.max_len = 56\n",
    "        self.word_idx_map = word_idx_map\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.xy)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        splitted_sentence = self.xy[idx]['text'].split()\n",
    "        tmp = []\n",
    "        for word in splitted_sentence:\n",
    "            tmp.append(self.word_idx_map[word])\n",
    "            \n",
    "        if len(tmp) < self.max_len:\n",
    "            for _ in range(len(tmp), self.max_len):\n",
    "                tmp.append(0)\n",
    "                \n",
    "        if self.xy[idx]['y'] in [0,1]: \n",
    "            return {'input_ids':torch.LongTensor(tmp).flatten(),\n",
    "                    'target':torch.tensor(self.xy[idx]['y'])}\n",
    "    \n",
    "        else:  # mr-dataset에는 여기에 걸리는 케이스가 없음. 전부 label 존재. \n",
    "            return {'input_ids':torch.LongTensor(tmp).flatten()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kfold_Split(revs, word_idx_map, test_fold_id):\n",
    "    train = []\n",
    "    test = []\n",
    "    for datum in revs:\n",
    "        if datum['split'] == test_fold_id:\n",
    "            test.append(datum)\n",
    "        else:\n",
    "            train.append(datum)   \n",
    "\n",
    "    train_dataset = Make_Dataset(word_idx_map=word_idx_map, xy=train)\n",
    "    test_dataset = Make_Dataset(word_idx_map=word_idx_map, xy=test)\n",
    "    \n",
    "    print('train length:', len(train_dataset))\n",
    "    print('test length:', len(test_dataset))\n",
    "## 우선 코드 전체적으로 쭈욱 한 번 돌리고 나서, 여길 수정하자.\n",
    "#     proper_batch_size = int(len(vocab)/10)\n",
    "#     print('proper:',proper_batch_size)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=50, shuffle=True, drop_last=True)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-trained vectors를 google negative300.bin으로 넣어주는 방법\n",
    "1) https://discuss.pytorch.org/t/expected-input-to-torch-embedding-layer-with-pre-trained-vectors-from-gensim/37029<br>\n",
    "2) static과 non-static의 구분을 nn.Embedding.from_pretrained(freeze=True/False)로 해줌 : https://github.com/aisolab/nlp_classification/blob/master/Convolutional_Neural_Networks_for_Sentence_Classification/model/ops.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Max-over-time pooling\n",
    "### max-over-time pooling 과 max-pooling의 다른 점\n",
    "# https://stackoverflow.com/questions/48549670/pooling-vs-pooling-over-time#:~:text=Max%20pooling%20typically%20applies%20to,along%20a%201d%20feature%20vector.&text=Max%20pooling%20over%20time%20takes,vector%20and%20computes%20the%20max.\n",
    "\n",
    "## origin\n",
    "# https://github.com/aisolab/nlp_classification/blob/master/Convolutional_Neural_Networks_for_Sentence_Classification/model/ops.py\n",
    "def MaxOverTimePooling(f3, f4, f5):   # retrieve each feature map\n",
    "    '''\n",
    "    input : f3, f4, f4   # 각각 filter=3일때, 4일때, 5일때\n",
    "          - each shape : batch_size, # of kernels, 3번째는 뭐지? \n",
    "          \n",
    "    process : f3.max = [3,6,9], (f3이 [[1,2,3],[4,5,6],[7,8,9]]일 경우)\n",
    "\n",
    "    output : 1-dim vector which containes a max value for each\n",
    "    '''\n",
    "    return torch.cat([\n",
    "            f3.max(dim=-1).values,\n",
    "            f4.max(dim=-1)[0],\n",
    "            f5.max(dim=-1)[0],\n",
    "        ], dim=-1,)\n",
    "\n",
    "\n",
    "## 1) 하단의 max_pool이 무엇을 받고,2) 어떤 연산을 하며, 3) 무엇을 내뱉는지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cnn_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Cnn_Model, self).__init__()\n",
    "        import gensim\n",
    "        self.n_filters = 100\n",
    "        self.input_dim = 1   # 그냥 text니깐 1임. vision일 경우 3 \n",
    "        ### word_vecs 만들어주기, dict 자료형에, word와 wordvector가 함께 있어야 함\n",
    "        word_vecs = gensim.models.KeyedVectors.load_word2vec_format(\"/kaggle/input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin\", binary=True, limit=700000)\n",
    "        self.word_vecs = torch.FloatTensor(word_vecs.vectors)\n",
    "        \n",
    "        ## nn.embedding은 2-dim float tensor로 만들어지고,\n",
    "        ## from_pretrained에서의 freeze는 기본적으로 True이다.\n",
    "        self.embedding = nn.Embedding.from_pretrained(self.word_vecs, freeze=False)\n",
    "        self.conv3_layer = nn.Conv2d(self.input_dim, self.n_filters, kernel_size=(3,300))\n",
    "        self.conv4_layer = nn.Conv2d(self.input_dim, self.n_filters, kernel_size=(4,300))\n",
    "        self.conv5_layer = nn.Conv2d(self.input_dim, self.n_filters, kernel_size=(5,300))\n",
    "        \n",
    "        ## 우선 filter_size를 3으로 줬을 때\n",
    "        self.fc = nn.Linear(3*self.n_filters, 1) \n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         print('first x:', x.size())                  # 50, 56                \n",
    "        x = self.embedding(x)                       # 50, 56, 300    \n",
    "#         print('second x:', x.size())        \n",
    "        x = x.unsqueeze(1)                          # 50, 1, 56,, 300                   \n",
    "#         print('third x:', x.size())\n",
    "        ## make a feature map for each filter\n",
    "        f3 = F.relu(self.conv3_layer(x).squeeze(3))  # 50, 10, 54\n",
    "        f4 = F.relu(self.conv4_layer(x).squeeze(3))  # 50, 10, 53\n",
    "        f5 = F.relu(self.conv5_layer(x).squeeze(3))  # 50, 10, 52\n",
    "        \n",
    "#### 원래 maxpooling 연산\n",
    "#         x3 = F.max_pool1d(f3, f3.shape[-1]).squeeze(-1)  # 50, 10  \n",
    "#                             # f3.shape[-1], 즉 54가 kernel_size로 들어감 \n",
    "#         x4 = F.max_pool1d(f4, f4.shape[-1]).squeeze(-1)  # 50, 10\n",
    "#         x5 = F.max_pool1d(f5, f5.shape[-1]).squeeze(-1)  # 50, 10\n",
    "#### 근데 max-over-time pooling과 significant한 차이는 없는거 같다..\n",
    "\n",
    "        x = MaxOverTimePooling(f3,f4,f5)\n",
    "    \n",
    "        ### a penultimate layer\n",
    "        x = self.dropout(x) * 0.5 # 50, 30\n",
    "        #TODO:ㅈㅈ 이게 test_loader가 돌아갈 땐 실행이 안 되어야 하는데...;;\n",
    "        \n",
    "#         print('sixth x:', x.size())\n",
    "#         x.size()  batch_size, n_filters*len(filter_sizes)\n",
    "\n",
    "        output = self.fc(x)                          # 50, 30\n",
    "#         print('seventh x:', x.size())   \n",
    "        \n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb\n",
    "# epochs = 25  # the value exists in original code\n",
    "epochs = 10  # adam 학습이 잘되서 더 적은 epoch으로도 괜찮을 듯\n",
    "model = Cnn_Model(word_vecs=word_vecs)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.BCELoss().to(device) \n",
    "# criterion = nn.NLLLoss().to(device) # NLL 썼음.. \n",
    "criterion = nn.BCEWithLogitsLoss().to(device) \n",
    "# optimizer = optim.Adadelta(model.parameters(), rho=0.95, eps=1e-6, weight_decay=0.95)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "for i in range(0,10):\n",
    "    note = {}\n",
    "    print('iiiiiiii:', i)\n",
    "    train_loader, test_loader = Kfold_Split(revs, word_idx_map, test_fold_id=i)\n",
    "      \n",
    "    ## Training\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        for idx, data in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(data['input_ids'].to(device))\n",
    "            loss = criterion(outputs, data['target'].type_as(outputs))\n",
    "            acc = binary_accuracy(outputs, data['target'].type_as(outputs))\n",
    "            loss.backward()   # 이걸로 W가 업뎃되나? 안될거 같은데. static/non-static설정 어케하나..\n",
    "            if tpu:\n",
    "                xm.optimizer_step(optimizer)\n",
    "            optimizer.step()\n",
    "\n",
    "            if idx % 50 == 0:\n",
    "                print('epoch:', epoch,' current acc:', acc)\n",
    "\n",
    "        print('training is done!')\n",
    "\n",
    "\n",
    "    ## test \n",
    "    total = len(test_loader)\n",
    "    correct = 0\n",
    "    model.eval()\n",
    "\n",
    "    ## .eval()이랑 no_grad()가 들어가면 자동으로 training=False가 되나?\n",
    "    ## .eval()에서 dropout은 disable됨\n",
    "    with torch.no_grad():\n",
    "        ### L2 norm weight clipping\n",
    "        for param in model.parameters():\n",
    "            param.clamp_(min=-3, max=3)\n",
    "            \n",
    "        ### 위의 param.clamp_ 이게 gradient update를 못하게 하기 때문에\n",
    "        ### train_loader에서는 쓰면 에러가 나는 것 같다.\n",
    "        \n",
    "        for data in test_loader:\n",
    "#             print('THIS IS THE DATA!:', data.size())\n",
    "#             print()\n",
    "            datas = data['input_ids'].to(device)\n",
    "#             print('datas size:', datas.size())  # 50.56\n",
    "            outputs = model(datas)\n",
    "#             print('outputs size:', outputs.size()) # the value should be [50,]\n",
    "#             print('outputs unsq', outputs.unsqueeze(1))\n",
    "            \n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "            labels = data['target'].to(device)\n",
    "            \n",
    "            acc = binary_accuracy(outputs, labels.type_as(outputs))\n",
    "            print('test accuracy:', acc)\n",
    "\n",
    "    ### 이렇게 구찮은거 말고, stratifiedKFold 써서\n",
    "    ### 글구 필터도 한 방에 해서 갈 수 있는거 알아보자..\n",
    "    # https://github.com/aisolab/nlp_classification/blob/master/Convolutional_Neural_Networks_for_Sentence_Classification/model/ops.py'\n",
    "    \n",
    "    ## w는 coefficient다\n",
    "    \n",
    "    # 결과 accuracy가 무슨 1에  가까운데.. 아무래도 네트워크가 데이터를 전부 외워버린듯 하다.\n",
    "    ## 아니면 코드상에서 무슨 잘못이 있었거나.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
